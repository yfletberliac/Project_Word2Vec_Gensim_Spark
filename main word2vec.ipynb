{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv( \"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the number of reviews that were read (100,000 in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
    " \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
    " test[\"review\"].size, unlabeled_train[\"review\"].size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert a document to a sequence of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist( review ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Remove stop words (false by default)\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split a review into parsed sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://www.nltk.org/nltk_data/\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "nltk.download()   \n",
    "\n",
    "# Load the tokenizer from nltk\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "Parsing sentences from unlabeled set\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(unicode(review, 'utf-8'), tokenizer)\n",
    "\n",
    "print \"Parsing sentences from unlabeled set\"\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(unicode(review, 'utf-8'), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n"
     ]
    }
   ],
   "source": [
    "print len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'some', u'may', u'call', u'mj', u'an', u'egotist', u'for', u'consenting', u'to', u'the', u'making', u'of', u'this', u'movie', u'but', u'mj', u'and', u'most', u'of', u'his', u'fans', u'would', u'say', u'that', u'he', u'made', u'it', u'for', u'the', u'fans', u'which', u'if', u'true', u'is', u'really', u'nice', u'of', u'him', u'the', u'actual', u'feature', u'film', u'bit', u'when', u'it', u'finally', u'starts', u'is', u'only', u'on', u'for', u'minutes', u'or', u'so', u'excluding', u'the', u'smooth', u'criminal', u'sequence', u'and', u'joe', u'pesci', u'is', u'convincing', u'as', u'a', u'psychopathic', u'all', u'powerful', u'drug', u'lord']\n"
     ]
    }
   ],
   "source": [
    "print sentences[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set values for various parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We don't plan to train the model any further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x171fb8550>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tree'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child tree daughter\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'brilliant', 0.6416057348251343),\n",
       " (u'wonderful', 0.6375398635864258),\n",
       " (u'superb', 0.6132605671882629),\n",
       " (u'amazing', 0.6075152158737183),\n",
       " (u'great', 0.6044968366622925),\n",
       " (u'terrific', 0.5843934416770935),\n",
       " (u'excellent', 0.5677196383476257),\n",
       " (u'fabulous', 0.563037633895874),\n",
       " (u'marvelous', 0.5626873970031738),\n",
       " (u'awesome', 0.5597708225250244)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"fantastic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'queen', 0.4603006839752197),\n",
       " (u'lion', 0.40316981077194214),\n",
       " (u'princess', 0.39915820956230164),\n",
       " (u'aladdin', 0.3486478626728058),\n",
       " (u'kong', 0.3401307463645935),\n",
       " (u'prince', 0.32044780254364014),\n",
       " (u'witch', 0.3197129964828491),\n",
       " (u'belle', 0.30830180644989014),\n",
       " (u'du', 0.3080085217952728),\n",
       " (u'madeleine', 0.3068543076515198)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compute clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16490, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06488955, -0.07009859,  0.00692126, -0.03094291,  0.05867873,\n",
       "        0.05126816,  0.11081391, -0.02649415,  0.02058307,  0.05279991,\n",
       "       -0.00633742, -0.06086138, -0.04158127,  0.0055691 , -0.02870898,\n",
       "       -0.16036388,  0.06647004, -0.03284989, -0.04153325, -0.04816008,\n",
       "       -0.11241508, -0.02980684, -0.00244578,  0.07241758,  0.03970441,\n",
       "        0.06498393,  0.10887307, -0.05738701, -0.05569573,  0.00159743,\n",
       "        0.03683783,  0.13546272,  0.00915493, -0.00978644, -0.00554668,\n",
       "        0.12002576,  0.02318511, -0.01718634, -0.02011414, -0.17007081,\n",
       "       -0.00674725,  0.02484204,  0.01301769, -0.04426865, -0.039248  ,\n",
       "       -0.01579146,  0.0019246 ,  0.17274301, -0.05032826,  0.00842139,\n",
       "       -0.08518724,  0.03673296, -0.0671815 , -0.04935077,  0.00463567,\n",
       "        0.03251321, -0.02030898,  0.04250539, -0.05616349, -0.02230252,\n",
       "        0.02729607,  0.07848719, -0.06396292,  0.05632715,  0.00628025,\n",
       "       -0.15204202, -0.00303472,  0.09669513,  0.13205555, -0.06921727,\n",
       "        0.00339411,  0.04754389,  0.02953019, -0.02590108, -0.00754221,\n",
       "        0.0109332 , -0.11105128,  0.00481792, -0.06484436, -0.05180406,\n",
       "        0.1127211 ,  0.08555473, -0.07737219,  0.0053693 , -0.02343424,\n",
       "       -0.00185327, -0.09024018,  0.00343372,  0.03218309,  0.01211335,\n",
       "       -0.01013358, -0.01985412, -0.02605802, -0.03958941,  0.02523773,\n",
       "       -0.15880735, -0.02153657,  0.06339897, -0.05407008,  0.00287781,\n",
       "        0.04576371,  0.02300273,  0.06451917,  0.02586523, -0.01678304,\n",
       "       -0.15777163, -0.08434693, -0.00281046, -0.00462078,  0.00581119,\n",
       "       -0.01618036, -0.03633288,  0.01447561,  0.01728767, -0.15220454,\n",
       "       -0.11220385, -0.0480766 , -0.03537789, -0.03248525,  0.05346294,\n",
       "        0.06878714,  0.06057437, -0.07976289,  0.073777  , -0.01673318,\n",
       "       -0.07005131, -0.0759687 , -0.01319254, -0.08626829, -0.0555019 ,\n",
       "        0.00701726, -0.00793046,  0.00155672, -0.06611915,  0.00741869,\n",
       "       -0.01522026, -0.04308198, -0.0210774 , -0.01892232, -0.07717688,\n",
       "        0.08768644, -0.07347067,  0.00502228,  0.0076593 ,  0.08051915,\n",
       "        0.00507024, -0.07134368,  0.06581403,  0.00903275,  0.02510458,\n",
       "        0.0382383 , -0.00820148,  0.0475812 , -0.0782425 , -0.10364632,\n",
       "        0.02251995, -0.00613567, -0.05017443, -0.07094391, -0.01023692,\n",
       "       -0.09052417,  0.01561085,  0.10140584,  0.09357882, -0.03313362,\n",
       "        0.05456708, -0.02296126, -0.06325285,  0.02174868,  0.02122048,\n",
       "       -0.07793311, -0.01918274, -0.01785455, -0.09084024,  0.10192532,\n",
       "       -0.0377434 , -0.04932018,  0.038867  ,  0.02912644,  0.01072283,\n",
       "        0.0980121 , -0.09258318, -0.04232112,  0.06051735,  0.00066073,\n",
       "       -0.03351506,  0.05346808, -0.05399195, -0.05950302, -0.01534513,\n",
       "        0.06603929, -0.06537664,  0.03788065,  0.06907377,  0.04890236,\n",
       "        0.04078817,  0.03493601, -0.02850673, -0.03065658, -0.05220466,\n",
       "        0.00481142, -0.02748276, -0.08436809,  0.01453649, -0.01001191,\n",
       "       -0.04753828, -0.0934737 ,  0.10070292, -0.0159418 ,  0.07273239,\n",
       "        0.04227597, -0.02240147, -0.02125021,  0.02415227,  0.08784421,\n",
       "       -0.01613824,  0.01002022,  0.03323234,  0.06555124, -0.15171647,\n",
       "        0.02574736,  0.05998013, -0.03485638, -0.01423876,  0.0273835 ,\n",
       "       -0.03380663,  0.02148602, -0.06475405,  0.01195521,  0.00372322,\n",
       "        0.02377676, -0.01069414,  0.04665019,  0.01787225, -0.06177269,\n",
       "       -0.03791703, -0.04133634, -0.05415002, -0.09100153, -0.00315148,\n",
       "        0.05106692,  0.13846792,  0.03966493,  0.0145866 , -0.07089394,\n",
       "       -0.06798666, -0.04458157,  0.04485147,  0.04607986, -0.00862709,\n",
       "        0.03175106, -0.0308718 ,  0.05635392,  0.04460427,  0.04957322,\n",
       "        0.05892437, -0.04190179,  0.05924032,  0.01415865, -0.10512378,\n",
       "        0.02587025, -0.05350946, -0.13811049,  0.01205921, -0.05368607,\n",
       "        0.01718744, -0.02066686,  0.05259142,  0.08105551, -0.01879937,\n",
       "       -0.0523326 , -0.10739989, -0.04365776,  0.04032127, -0.02220787,\n",
       "        0.05638975, -0.04160845, -0.02476257, -0.03139061,  0.04135919,\n",
       "       -0.01249376,  0.07579045,  0.05694601,  0.02689026, -0.00473064,\n",
       "       -0.01892803, -0.06103719, -0.00336145,  0.01310611,  0.0672235 ,\n",
       "       -0.02883806, -0.01830184,  0.01500024, -0.01902016, -0.05670033,\n",
       "        0.0003249 ,  0.01792605, -0.04983172, -0.03651597,  0.02460518], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"flower\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set values for various parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectors = model.syn0\n",
    "num_clusters = word_vectors.shape[0] / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  1116.1862781 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/10th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.syn0\n",
    "num_clusters = word_vectors.shape[0] / 10\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print \"Time taken for K Means clustering: \", elapsed, \"seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "f = file('idx.save', 'wb')\n",
    "cPickle.dump(idx, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "from numpy import array\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "# -----\n",
    "kmeans_clustering_spark = KMeans.train(sc.parallelize(word_vectors), num_clusters, \\\n",
    "                                 maxIterations=3, runs=30, initializationMode=\"random\")\n",
    "print \"Processing predict\"\n",
    "idx_spark = kmeans_clustering_spark.predict(word_vectors)\n",
    "# -----\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print \"Time taken for K Means Spark clustering: \", elapsed, \"seconds.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = file('idx.save', 'rb')\n",
    "idx = cPickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping a word to its cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_centroid_map = dict(zip( model.index2word, idx ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "[u'tendency']\n",
      "\n",
      "Cluster 1\n",
      "[u'krueger', u'kruger', u'glove', u'englund']\n",
      "\n",
      "Cluster 2\n",
      "[u'achilles', u'casualty']\n",
      "\n",
      "Cluster 3\n",
      "[u'shambles', u'ineffective', u'futile', u'ploy', u'moralistic', u'lumbering', u'curiously', u'unwelcome', u'uber', u'deathly', u'squarely', u'heinous', u'descended', u'catastrophe', u'contention', u'handedly']\n",
      "\n",
      "Cluster 4\n",
      "[u'inadvertently', u'disguise', u'stealing', u'cooks', u'clutches', u'threatens', u'unsuccessfully', u'ruse', u'suicidal', u'fakes', u'lawless', u'colleagues', u'vegetarian', u'sends', u'pose', u'unsuccessful', u'faking', u'overnight', u'kindly', u'threatening', u'threaten', u'crosses', u'kidnap', u'unemployed', u'therapy', u'blackmail', u'bets', u'deed', u'seduce', u'goody', u'cure', u'resurrect', u'lure', u'ridiculed', u'sells', u'satanist', u'prefers', u'complains', u'caller', u'serum', u'urges', u'execute']\n",
      "\n",
      "Cluster 5\n",
      "[u'dismissed', u'banned', u'hindsight', u'panned', u'critics', u'hype', u'recognised', u'praised', u'label', u'remade', u'criticized', u'referenced', u'censors', u'trashed', u'moviegoers', u'lommel']\n",
      "\n",
      "Cluster 6\n",
      "[u'astonishing', u'amazing', u'ensemble', u'extraordinary', u'exceptional', u'incredible']\n",
      "\n",
      "Cluster 7\n",
      "[u'hello', u'concerts', u'es', u'beta', u'vinyl', u'easter', u'waltz']\n",
      "\n",
      "Cluster 8\n",
      "[u'ensuing', u'recalling', u'erupts', u'parting', u'hectic', u'heated', u'gleefully', u'gunfight', u'fading', u'drifts', u'hurricane', u'disposed', u'wipes', u'unscathed', u'bounces', u'mishaps', u'interrogation']\n",
      "\n",
      "Cluster 9\n",
      "[u'scary']\n",
      "\n",
      "Cluster 10\n",
      "[u'rico', u'explorer', u'prototype', u'intrepid', u'racer', u'rode', u'toughest', u'nosed', u'yu', u'protector', u'gi', u'poet', u'bison', u'daredevil', u'aptly', u'conducted', u'copper', u'judas', u'penguin', u'gunslinger', u'owl']\n",
      "\n",
      "Cluster 11\n",
      "[u'echoes', u'photographic', u'sumptuous', u'lovingly', u'utilizing', u'muted', u'varied', u'rendering', u'sweeping', u'reflected', u'composition', u'cinemascope', u'mastery', u'distinctive', u'lyrical', u'moods', u'showcasing', u'gloss', u'maintained', u'evocative', u'matching', u'decoration', u'vistas', u'coloring', u'enhanced', u'compositions', u'artistry', u'fluid', u'splendor', u'verve', u'backdrops', u'utilizes', u'complement', u'grandeur']\n",
      "\n",
      "Cluster 12\n",
      "[u'divided', u'unexplored', u'regions', u'venue', u'ventures', u'mumbai']\n",
      "\n",
      "Cluster 13\n",
      "[u'boyhood', u'sol', u'papa', u'ivy', u'barber', u'hick', u'proprietor', u'hostess', u'pier', u'mechanic', u'salon', u'tavern', u'peeping', u'antique', u'prairie', u'kiki', u'squire', u'operator', u'rudy', u'sammi', u'paolo', u'ga', u'specialty', u'brock', u'janitor', u'mat', u'waiter', u'crane', u'den', u'landlord', u'beetle', u'electronics', u'parks', u'mercedes', u'heel', u'della', u'heavenly', u'bake', u'district', u'runaway', u'saloon', u'instructor', u'johns', u'conductor', u'diego']\n",
      "\n",
      "Cluster 14\n",
      "[u'hopelessly', u'unremarkable', u'insufferable']\n",
      "\n",
      "Cluster 15\n",
      "[u'attracts', u'applies', u'shares', u'inspires', u'describes', u'confirms', u'relates', u'possesses', u'embodies', u'eloquent', u'supports', u'storyteller']\n",
      "\n",
      "Cluster 16\n",
      "[u'shawn', u'benny', u'carson', u'willie', u'wallace', u'justin', u'terry', u'herman', u'beery', u'harrison', u'kennedy', u'gregory', u'austin']\n",
      "\n",
      "Cluster 17\n",
      "[u'copious', u'unintended', u'reduce', u'doses', u'nastiness', u'chills', u'plentiful', u'laden', u'adrenaline', u'amounts', u'abundance', u'concoction', u'dose', u'vulgarity', u'bloodshed', u'moderate', u'resorting', u'titillation', u'stream', u'visceral']\n",
      "\n",
      "Cluster 18\n",
      "[u'appreciate', u'care', u'explain', u'believe', u'understand', u'know']\n",
      "\n",
      "Cluster 19\n",
      "[u'busy', u'wandering', u'aimlessly', u'around', u'wander', u'running']\n"
     ]
    }
   ],
   "source": [
    "# For the first 10 clusters\n",
    "for cluster in xrange(0,20):\n",
    "    #\n",
    "    # Print the cluster number  \n",
    "    print \"\\nCluster %d\" % cluster\n",
    "    #\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for i in xrange(0,len(word_centroid_map.values())):\n",
    "        if( word_centroid_map.values()[i] == cluster ):\n",
    "            words.append(word_centroid_map.keys()[i])\n",
    "    print words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
